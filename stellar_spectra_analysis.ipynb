{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stellar Parameter and Abundance Estimation using ML\n",
    "\n",
    "## Introduction:\n",
    "\n",
    "This project began as a senior honours dissertation to analyse Milky Way Survey (MWS) spectra from the Dark Energy Spectroscopic Instrument (DESI) and estimate stellar parameters ‚Äî effective temperature ($T_\\text{eff}$), surface gravity ($\\log g$), and metallicity [Fe/H] ‚Äî along with detailed chemical abundances, using machine learning.\n",
    "\n",
    "This work is essential for improving current techniques used to infer these stellar quantities from observed spectra. Better estimates lead to more accurate models of stellar populations and a deeper understanding of Galactic structure and evolution, particularly how stellar populations differ based on their location within the Milky Way.\n",
    "\n",
    "We apply two main ML strategies:\n",
    "\n",
    "1. Random Forest Regression with PCA for dimensionality reduction.\n",
    "\n",
    "2. Neural Networks for learning complex mappings from spectra to parameters.\n",
    "\n",
    "Future work includes integrating physics-informed loss functions to guide the models using astrophysical priors and improve physical reliability.\n",
    "\n",
    "This notebook walks through:\n",
    "\n",
    "- Preprocessing the DESI spectra (e.g., Doppler correction, flux normalization),\n",
    "\n",
    "- Training and evaluating ML models for parameter estimation,\n",
    "\n",
    "- Visualizing model performance and residuals,\n",
    "\n",
    "- Discussing implications and next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import load\n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "# load configuration\n",
    "from utils import load_config, setup_env\n",
    "setup_env(load_config('config.yaml'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç 3. Data Preprocessing\n",
    "\n",
    "Preprocessing is a crucial step in this project to ensure the spectral data is clean, consistent, and suitable for input into machine learning models. The raw data consists of DESI **FITS files** containing flux measurements from three cameras (**B**, **R**, **Z**) across different wavelength ranges.\n",
    "\n",
    "The preprocessing pipeline involves the following steps:\n",
    "\n",
    "---\n",
    "\n",
    "### üìÅ 3.1 Load FITS Spectra\n",
    "\n",
    "- The FITS files are loaded using `astropy.io.fits`, one for each DESI observation.\n",
    "- Each file contains flux, wavelength, and masking information for three spectral bands:\n",
    "  - **B-band**: blue wavelengths\n",
    "  - **R-band**: red wavelengths\n",
    "  - **Z-band**: infrared wavelengths\n",
    "\n",
    "Each FITS file also contains metadata (RA/Dec) needed to cross-match with label data.\n",
    "\n",
    "---\n",
    "\n",
    "### üåå 3.2 Cross-Match with APOGEE Catalog\n",
    "\n",
    "To obtain ground-truth labels (e.g. [Fe/H], $\\log g$), the DESI targets are **matched by sky coordinates** with high-fidelity **APOGEE DR17** data:\n",
    "- RA/Dec from both catalogs are compared using `astropy.coordinates.SkyCoord`.\n",
    "- Matches within **3 arcseconds** are accepted.\n",
    "- For each matched star, the relevant **chemical abundance values** are extracted and stored.\n",
    "\n",
    "This step ensures the spectra and the training labels refer to the **same stars**.\n",
    "\n",
    "---\n",
    "\n",
    "### üßº 3.3 Wavelength Alignment & Trimming\n",
    "\n",
    "The different cameras overlap slightly, but for consistency:\n",
    "- The last **25 pixels** of the **B-band**, the first **26 pixels** and last **63 pixels** of the **R-band**, and the first **63 pixels** of the **Z-band** are removed.\n",
    "- This avoids noisy or edge-effect-prone regions and creates a continuous 1D wavelength grid.\n",
    "\n",
    "All cameras are then **stitched together** into one full spectrum.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è 3.4 Masking and Interpolation\n",
    "\n",
    "Each camera includes a **mask array** indicating bad pixels. These pixels are:\n",
    "- Marked as masked values (`np.ma.MaskedArray`)\n",
    "- Interpolated using a 1D linear interpolation based on neighboring valid pixels\n",
    "\n",
    "> This step ensures **no NaNs or masked values** remain in the flux array, which is critical for ML input.\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ 3.5 Doppler Shift Correction\n",
    "\n",
    "Each star's **radial velocity** ($v_\\text{helio}$) from APOGEE is used to correct for Doppler shifts:\n",
    "- The observed wavelengths are shifted to the **rest frame** using:\n",
    "\n",
    "  $$ \\lambda_\\text{rest} = \\frac{\\lambda_\\text{obs}}{1 + v/c} $$\n",
    "\n",
    "- The flux is interpolated to a **common rest-frame wavelength grid**, ensuring consistency across stars.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÉ 3.6 Flux Normalization\n",
    "\n",
    "The rest-frame flux is then **continuum-normalized**:\n",
    "- A **moving median filter** (window size = 151) is applied to approximate the continuum.\n",
    "- The flux is divided by this smoothed continuum, reducing the impact of instrumental effects and star-specific baselines.\n",
    "\n",
    "---\n",
    "\n",
    "### üíæ 3.7 Save Processed Data\n",
    "\n",
    "- All normalized, rest-frame flux arrays are stacked into a single matrix: `flux.joblib`, shape = (n_stars, n_pixels).\n",
    "- Corresponding labels (stellar parameters and abundances) are saved in `labels.csv`.\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ Example: Plotting a Normalized Spectrum\n",
    "\n",
    "```python\n",
    "from joblib import load\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "flux = load(\"data/spectral_dir/flux.joblib\")\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(flux[0])\n",
    "plt.title(\"Normalized, Rest-Frame Spectrum (Star 0)\")\n",
    "plt.xlabel(\"Wavelength Index\")\n",
    "plt.ylabel(\"Normalized Flux\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ 4. Modeling\n",
    "\n",
    "The goal of this stage is to map the **preprocessed spectral flux** of a star to its **stellar parameters** (e.g., $T_\\text{eff}$, $\\log g$, [Fe/H]) and **chemical abundances** (e.g., [C/Fe], [Mg/Fe], etc.) using machine learning models.\n",
    "\n",
    "Two different types of models are trained and evaluated:\n",
    "\n",
    "- üå≤ **Random Forest Regression** with **Incremental PCA** (for dimensionality reduction)\n",
    "- üß† **Feedforward Neural Networks** tuned using KerasTuner\n",
    "\n",
    "---\n",
    "\n",
    "### üå≤ 4.1 Random Forest + PCA\n",
    "\n",
    "High-dimensional spectral data (thousands of wavelength points) poses challenges for classical models. To address this:\n",
    "\n",
    "- We use **Incremental PCA (IPCA)** to reduce flux dimensionality while preserving variance.\n",
    "- Then, we apply **Random Forest Regression** ‚Äî a robust, interpretable ensemble model.\n",
    "\n",
    "This is implemented as a **`sklearn.Pipeline`**, which includes:\n",
    "1. `StandardScaler()` for feature scaling,\n",
    "2. `IncrementalPCA(n_components=100)` for reducing dimensions,\n",
    "3. `RandomForestRegressor()` for prediction.\n",
    "\n",
    "Hyperparameters such as the number of PCA components, tree depth, and leaf size are tuned using **`GridSearchCV`** with 5-fold cross-validation.\n",
    "\n",
    "#### üõ† Example: Load a trained model and predict Teff\n",
    "\n",
    "```python\n",
    "from joblib import load\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import pandas as pd\n",
    "\n",
    "flux = load(\"data/spectral_dir/flux.joblib\")\n",
    "labels = pd.read_csv(\"data/label_dir/labels.csv\")\n",
    "y_true = labels[\"teff\"].to_numpy()\n",
    "mask = ~np.isnan(y_true)\n",
    "\n",
    "model = load(\"models/teff_model.joblib\")\n",
    "y_pred = model.predict(flux[mask])\n",
    "\n",
    "rmse = root_mean_squared_error(y_true[mask], y_pred)\n",
    "print(f\"Teff RMSE (Random Forest): {rmse:.2f} K\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üß† 4.2 Neural Network (NN) Models\n",
    "\n",
    "To model non-linear relationships in the spectral data, we also train **feedforward neural networks (FNNs)** using **TensorFlow/Keras**.\n",
    "\n",
    "The neural network architecture is selected using **KerasTuner**, which explores:\n",
    "- Number of hidden units (32‚Äì128),\n",
    "- Dropout rate (0‚Äì0.5),\n",
    "- Learning rate (1e-4 to 1e-2).\n",
    "\n",
    "**Architecture Summary:**\n",
    "- Input layer = flattened flux vector\n",
    "- 2 hidden layers (ReLU activations, Dropout optional)\n",
    "- Output layer = scalar (parameter or abundance value)\n",
    "\n",
    "Training uses:\n",
    "- `Adam` optimizer\n",
    "- `EarlyStopping` and `ReduceLROnPlateau` callbacks\n",
    "- Loss = mean squared error\n",
    "\n",
    "#### üõ† Example: Predict with trained NN model\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "flux = load(\"data/spectral_dir/flux.joblib\")\n",
    "labels = pd.read_csv(\"data/label_dir/labels.csv\")\n",
    "y_true = labels[\"teff\"].to_numpy()\n",
    "mask = ~np.isnan(y_true)\n",
    "\n",
    "model = load_model(\"models/teff_model.keras\")\n",
    "y_pred = model.predict(flux[mask]).flatten()\n",
    "\n",
    "rmse = root_mean_squared_error(y_true[mask], y_pred)\n",
    "print(f\"Teff RMSE (Neural Network): {rmse:.2f} K\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üìâ 4.3 Residual Analysis & Model Comparison\n",
    "\n",
    "For each parameter, we compute:\n",
    "- **Predicted value**\n",
    "- **Residual** = true value ‚Äì predicted value\n",
    "- **RMSE**: Root Mean Squared Error\n",
    "\n",
    "These are visualized to assess systematic offsets and model uncertainty.\n",
    "\n",
    "#### üõ† Example: Plot residuals vs. [Fe/H]\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "residuals = pd.read_csv(\"residuals/teff.csv\")\n",
    "\n",
    "plt.scatter(residuals[\"[Fe/H]\"], residuals[\"Residuals\"], alpha=0.4)\n",
    "plt.axhline(0, color='gray', linestyle='--')\n",
    "plt.title(\"Teff Residuals vs [Fe/H]\")\n",
    "plt.xlabel(\"[Fe/H]\")\n",
    "plt.ylabel(\"Teff Residuals (K)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ 4.4 Performance Summary\n",
    "\n",
    "A full summary of RMSE values across all parameters is saved in:\n",
    "\n",
    "```python\n",
    "errors = pd.read_csv(\"output/results/predictions_errors.csv\")\n",
    "errors.sort_values(\"RMSE\")\n",
    "```\n",
    "\n",
    "This helps identify which stellar parameters are easier/harder to predict, and which model (RF or NN) performs better for each.\n",
    "\n",
    "---\n",
    "\n",
    "Next, we‚Äôll move to final discussion and interpretation of these results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß¨ 5. Toward Physics-Informed Neural Networks (PINNs)\n",
    "\n",
    "While classical machine learning models (like Random Forests and standard Neural Networks) can effectively learn complex mappings between spectral features and stellar labels, they do so **agnostically** ‚Äî without knowing anything about the physics of stars. This can lead to predictions that are:\n",
    "\n",
    "- Physically inconsistent (e.g., [C/N] increasing with stellar age),\n",
    "- Poorly generalizable in low-data regimes,\n",
    "- Sensitive to spurious correlations.\n",
    "\n",
    "To address this, we are exploring the use of **Physics-Informed Neural Networks (PINNs)**.\n",
    "\n",
    "---\n",
    "\n",
    "### ü§î What is a PINN?\n",
    "\n",
    "A **Physics-Informed Neural Network** incorporates **domain-specific physical constraints** into the loss function of a neural network. Instead of minimizing error purely based on training labels (supervised loss), the model is also penalized for violating **physical laws or empirical relationships**.\n",
    "\n",
    "In our case, candidate physics-based constraints include:\n",
    "\n",
    "| Constraint | Rationale |\n",
    "|-----------|-----------|\n",
    "| Spectral Flux Conservation | Total energy across wavelength range should be bounded |\n",
    "| [C/N] vs. log(g) or age | Carbon and nitrogen abundances evolve predictably in red giants |\n",
    "| Known stellar relations (e.g., $\\log g \\propto M/R^2$) | Consistency with stellar structure theory |\n",
    "| Abundance gradients vs. [Fe/H] | Enforcing chemical evolution trends |\n",
    "\n",
    "---\n",
    "\n",
    "### üèó PINN Loss Function Design\n",
    "\n",
    "The total loss $\\mathcal{L}_{\\text{total}}$ combines:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{data}} + \\lambda_{\\text{phys}} \\cdot \\mathcal{L}_{\\text{physics}}\n",
    "$$\n",
    "\n",
    "- **$\\mathcal{L}_{\\text{data}}$**: standard MSE between predicted and true values.\n",
    "- **$\\mathcal{L}_{\\text{physics}}$**: penalty for violating known astrophysical relations.\n",
    "- **$\\lambda_{\\text{phys}}$**: a tunable weight controlling how strongly physics is enforced.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Implementation Roadmap\n",
    "\n",
    "Here‚Äôs the plan for integrating PINNs into this project:\n",
    "\n",
    "1. **Define Physics Constraints**  \n",
    "   E.g., use [C/N] vs. $\\log g$ relation as a differentiable loss term.\n",
    "\n",
    "2. **Extend Neural Network Training Code**  \n",
    "   Modify `train_neural_network()` to include a custom loss function combining MSE and physics loss.\n",
    "\n",
    "3. **Experiment with Œª Weighting**  \n",
    "   Perform hyperparameter tuning to find a balance between data fit and physics regularization.\n",
    "\n",
    "4. **Evaluate Performance & Interpretability**  \n",
    "   Assess whether PINNs reduce residuals, improve physical trends, and generalize better on edge cases.\n",
    "\n",
    "---\n",
    "\n",
    "### üìò Example: Adding a Physics Term\n",
    "\n",
    "Here's a conceptual sketch of what a physics-informed loss could look like in TensorFlow:\n",
    "\n",
    "```python\n",
    "def physics_loss(y_pred, features):\n",
    "    # e.g., penalize unphysical [C/N] values at low logg\n",
    "    logg = features[\"logg\"]\n",
    "    c_n = y_pred[\"c_fe\"] - y_pred[\"n_fe\"]\n",
    "    penalty = tf.nn.relu(c_n + 0.5 * (logg - 1.5))  # Toy example\n",
    "    return tf.reduce_mean(penalty)\n",
    "\n",
    "def total_loss(y_true, y_pred, features, lambda_phys=0.1):\n",
    "    mse = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    phys = physics_loss(y_pred, features)\n",
    "    return mse + lambda_phys * phys\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Why PINNs Matter\n",
    "\n",
    "Incorporating physics:\n",
    "- Increases **model interpretability**,\n",
    "- Reduces **overfitting to noisy training labels**,\n",
    "- Makes the models more **robust to outliers and extrapolation**,\n",
    "- Helps bridge **theory and data-driven methods** in stellar astrophysics.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÆ Future Work\n",
    "\n",
    "- Incorporate more complex constraints (e.g., HR diagram priors, stellar evolution tracks).\n",
    "- Apply PINNs to low-S/N spectra and rare stellar populations.\n",
    "- Benchmark PINN models vs. standard NNs across different galactic environments.\n",
    "\n",
    "---\n",
    "\n",
    "PINNs represent an exciting next step in combining astrophysical knowledge with the flexibility of deep learning. Work is ongoing to prototype and evaluate their full integration in this framework.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 6. Results & Evaluation\n",
    "\n",
    "After training both **Random Forest** and **Neural Network** models, we assess their performance on a set of stellar parameters and chemical abundances. The key metric used for evaluation is the **Root Mean Squared Error (RMSE)**:\n",
    "\n",
    "$$\n",
    "\\text{RMSE} = \\sqrt{ \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2 }\n",
    "$$\n",
    "\n",
    "This gives a direct measure of prediction error in physical units (e.g., Kelvin for $T_\\text{eff}$, dex for abundances).\n",
    "\n",
    "---\n",
    "\n",
    "### üìã 6.1 Summary of Model Performance\n",
    "\n",
    "The table below lists the RMSE values for each parameter:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "rf_results = pd.read_csv(\"output/results/predictions_errors.csv\")  # Random Forest\n",
    "rf_results[\"Model\"] = \"Random Forest\"\n",
    "\n",
    "nn_results = pd.read_csv(\"results/predictions_errors.csv\")  # Neural Network\n",
    "nn_results[\"Model\"] = \"Neural Network\"\n",
    "\n",
    "summary = pd.concat([rf_results, nn_results])\n",
    "summary = summary.pivot(index=\"Parameter\", columns=\"Model\", values=\"RMSE\").sort_index()\n",
    "summary\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öñÔ∏è 6.2 Random Forest vs Neural Network\n",
    "\n",
    "This bar plot compares the RMSE for each parameter across both model types:\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "summary.plot(kind='bar', figsize=(14, 6))\n",
    "plt.title(\"Model Performance: Random Forest vs Neural Network\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(True, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "> üìå **Observation**: For parameters like $T_\\text{eff}$ and [Fe/H], both models may perform similarly. For trace elements (e.g., [C/Fe], [Ti/Fe]), neural networks may capture more subtle features due to non-linearity.\n",
    "\n",
    "---\n",
    "\n",
    "### üìâ 6.3 Residual Analysis\n",
    "\n",
    "We visualize the residuals (true - predicted) for a few key parameters to identify systematic biases or model failures.\n",
    "\n",
    "#### üîé Example: Teff Residuals\n",
    "\n",
    "```python\n",
    "res = pd.read_csv(\"residuals/teff.csv\")\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(res[\"[Fe/H]\"], res[\"Residuals\"], alpha=0.4)\n",
    "plt.axhline(0, color=\"black\", linestyle=\"--\")\n",
    "plt.title(\"Teff Residuals vs [Fe/H]\")\n",
    "plt.xlabel(\"[Fe/H]\")\n",
    "plt.ylabel(\"Residuals (K)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "You can repeat this for other parameters like `logg`, `[Mg/Fe]`, etc., to spot patterns.\n",
    "\n",
    "---\n",
    "\n",
    "### üåç 6.4 Trends vs Metallicity\n",
    "\n",
    "Plotting predictions against [Fe/H] can reveal physical consistency:\n",
    "\n",
    "```python\n",
    "teff_preds = pd.read_csv(\"results/teff_predictions.csv\")\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(teff_preds[\"[Fe/H]\"], teff_preds[\"teff\"], alpha=0.5, s=10)\n",
    "plt.title(\"Predicted Teff vs [Fe/H]\")\n",
    "plt.xlabel(\"[Fe/H]\")\n",
    "plt.ylabel(\"Teff (K)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üß† 6.5 General Observations\n",
    "\n",
    "- **Teff and [Fe/H]** are predicted with relatively low error, suggesting strong signal in the flux.\n",
    "- **Surface gravity ($\\log g$)** is often harder to estimate and may benefit from PINN regularization.\n",
    "- **Abundance predictions** (e.g., [C/Fe], [Mg/Fe]) show greater spread, particularly for rare elements or low-S/N spectra.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ 6.6 Evaluation Summary\n",
    "\n",
    "| Metric         | Comment |\n",
    "|----------------|---------|\n",
    "| **RMSE**       | Used to quantify overall error per parameter |\n",
    "| **Residuals**  | Help identify systematic under/overestimation |\n",
    "| **Trend plots**| Useful for checking astrophysical consistency |\n",
    "| **Model Comparison** | Neural networks often outperform RFs on more subtle parameters, but RFs are faster and more interpretable |\n",
    "\n",
    "---\n",
    "\n",
    "In the next section, we discuss scientific interpretation and future improvements ‚Äî including using Physics-Informed models to boost physical reliability.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
